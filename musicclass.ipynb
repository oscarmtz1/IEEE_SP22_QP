{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Music genre classifier with TensorFlow","metadata":{}},{"cell_type":"markdown","source":"Author - Sagnick Bhar \nThe objective of this project is to classify 30 sec wav files by genre using a TensorFLow LSTM model. The GTZAN dataset can be found here:\n\nhttps://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification\n\nTo classify audio samples, we will preprocess them by calculating their MFCC, which is a temporal representation of the energy for each perceived frequency band. In this case, we are choosing 13 bands.","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:22.061176Z","iopub.execute_input":"2022-05-01T14:44:22.06151Z","iopub.status.idle":"2022-05-01T14:44:27.802848Z","shell.execute_reply.started":"2022-05-01T14:44:22.061424Z","shell.execute_reply":"2022-05-01T14:44:27.802023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SOURCE_PATH = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/'\n\n# Path to labels and processed data file, json format.\nJSON_PATH = '/kaggle/working/data.json'\n\n# Sampling rate.\nsr = 22050\n\n# Let's make sure all files have the same amount of samples and pick a duration right under 30 seconds.\nTOTAL_SAMPLES = 29 * sr\n\n# The dataset contains 999 files. Lets make it bigger. \n# X amount of slices => X times more training examples.\nNUM_SLICES = 10\nSAMPLES_PER_SLICE = int(TOTAL_SAMPLES / NUM_SLICES)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:27.804425Z","iopub.execute_input":"2022-05-01T14:44:27.804668Z","iopub.status.idle":"2022-05-01T14:44:27.810665Z","shell.execute_reply.started":"2022-05-01T14:44:27.804634Z","shell.execute_reply":"2022-05-01T14:44:27.810009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(source_path, json_path):\n\n    # Let's create a dictionary of labels and processed data.\n    mydict = {\n        \"labels\": [],\n        \"mfcc\": []\n        }\n\n    # Let's browse each file, slice it and generate the 13 band mfcc for each slice.\n    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(source_path)):\n        for file in filenames:\n            # exclude a corrupted wav file that makes everything crash.\n            if os.path.join(dirpath, file) != '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav':\n                song, sr = librosa.load(os.path.join(dirpath, file), duration=29)\n                for s in range(NUM_SLICES):\n                    start_sample = SAMPLES_PER_SLICE * s\n                    end_sample = start_sample + SAMPLES_PER_SLICE\n                    mfcc = librosa.feature.mfcc(y=song[start_sample:end_sample], sr=sr, n_mfcc=13)\n                    mfcc = mfcc.T\n                    mydict[\"labels\"].append(i-1)\n                    mydict[\"mfcc\"].append(mfcc.tolist())\n            else:\n                pass\n\n    # Let's write the dictionary in a json file.    \n    with open(json_path, 'w') as f:\n        json.dump(mydict, f)\n    f.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:31.003619Z","iopub.execute_input":"2022-05-01T14:44:31.004251Z","iopub.status.idle":"2022-05-01T14:44:31.013902Z","shell.execute_reply.started":"2022-05-01T14:44:31.004214Z","shell.execute_reply":"2022-05-01T14:44:31.013155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(json_path):\n\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    f.close()\n\n    # Let's load our data into numpy arrays for TensorFlow compatibility.\n    X = np.array(data[\"mfcc\"])\n    y = np.array(data[\"labels\"])\n\n    return X, y","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:32.722919Z","iopub.execute_input":"2022-05-01T14:44:32.723219Z","iopub.status.idle":"2022-05-01T14:44:32.728823Z","shell.execute_reply.started":"2022-05-01T14:44:32.723181Z","shell.execute_reply":"2022-05-01T14:44:32.727988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_datasets(inputs, targets, split_size):\n    \n    # Creating a validation set and a test set.\n    inputs_train, inputs_val, targets_train, targets_val = train_test_split(inputs, targets, test_size=split_size)\n    inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs_train, targets_train, test_size=split_size)\n    \n    # Our CNN model expects 3D input shape.\n    inputs_train = inputs_train[..., np.newaxis]\n    inputs_val = inputs_val[..., np.newaxis]\n    inputs_test = inputs_test[..., np.newaxis]\n    \n    return inputs_train, inputs_val, inputs_test, targets_train, targets_val, targets_test","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:34.03731Z","iopub.execute_input":"2022-05-01T14:44:34.037911Z","iopub.status.idle":"2022-05-01T14:44:34.048331Z","shell.execute_reply.started":"2022-05-01T14:44:34.037869Z","shell.execute_reply":"2022-05-01T14:44:34.047393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def design_model(input_shape):\n\n    # Let's design the model architecture.\n    model = tf.keras.models.Sequential([\n        \n        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        \n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu'), \n        tf.keras.layers.Dense(len(np.unique(targets)), activation='softmax')\n    ])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:35.159093Z","iopub.execute_input":"2022-05-01T14:44:35.1597Z","iopub.status.idle":"2022-05-01T14:44:35.168315Z","shell.execute_reply.started":"2022-05-01T14:44:35.159654Z","shell.execute_reply":"2022-05-01T14:44:35.167603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_prediction(model, X, y, idx):\n    \n    genre_dict = {\n        0 : \"blues\",\n        1 : \"classical\",\n        2 : \"country\",\n        3 : \"disco\",\n        4 : \"hiphop\",\n        5 : \"jazz\",\n        6 : \"metal\",\n        7 : \"pop\",\n        8 : \"reggae\",\n        9 : \"rock\",\n        }\n        \n    predictions = model.predict(X)\n    genre = np.argmax(predictions[idx])\n    \n    print(\"\\n---Now testing the model for one audio file---\\nThe model predicts: {}, and ground truth is: {}.\\n\".format(genre_dict[genre], genre_dict[y[idx]]))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:44.680933Z","iopub.execute_input":"2022-05-01T14:44:44.681212Z","iopub.status.idle":"2022-05-01T14:44:44.687659Z","shell.execute_reply.started":"2022-05-01T14:44:44.681176Z","shell.execute_reply":"2022-05-01T14:44:44.686265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_performance(hist):\n    \n    acc = hist.history['acc']\n    val_acc = hist.history['val_acc']\n    loss = hist.history['loss']\n    val_loss = hist.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'r', label='Training accuracy')\n    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n\n    plt.plot(epochs, loss, 'r', label='Training Loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:46.550851Z","iopub.execute_input":"2022-05-01T14:44:46.551104Z","iopub.status.idle":"2022-05-01T14:44:46.557758Z","shell.execute_reply.started":"2022-05-01T14:44:46.551077Z","shell.execute_reply":"2022-05-01T14:44:46.556824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    preprocess_data(source_path=SOURCE_PATH, json_path=JSON_PATH)\n    \n    inputs, targets = load_data(json_path=JSON_PATH)\n    \n    Xtrain, Xval, Xtest, ytrain, yval, ytest = prepare_datasets(inputs, targets, 0.2)\n\n    input_shape = (Xtrain.shape[1], Xtrain.shape[2], 1)\n    model = design_model(input_shape)\n\n    # Selection of the optimizer, loss type and metrics for performance evaluation.\n    model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.001),\n                     loss='sparse_categorical_crossentropy',\n                     metrics = ['acc']\n                     )\n\n    model.summary()\n\n    #Training the model.\n    history = model.fit(Xtrain, ytrain,\n                        validation_data=(Xval, yval),\n                        epochs=30,\n                        batch_size=32\n                        )\n\n    plot_performance(history)\n\n    # Testing the model on never seen before data.\n    make_prediction(model, Xtest, ytest, 24)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T14:44:50.733919Z","iopub.execute_input":"2022-05-01T14:44:50.734472Z","iopub.status.idle":"2022-05-01T14:47:03.519318Z","shell.execute_reply.started":"2022-05-01T14:44:50.734431Z","shell.execute_reply":"2022-05-01T14:47:03.518259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The End**","metadata":{}}]}